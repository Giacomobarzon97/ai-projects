{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive Bayes on 20 user newsgroup.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Rx9JnoWXnXhj","colab_type":"text"},"source":["# Downloading data"]},{"cell_type":"code","metadata":{"id":"OOPQBLTJ_3zf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598189036815,"user_tz":-120,"elapsed":1664,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}}},"source":["from sklearn.datasets import fetch_20newsgroups\n","train_data = fetch_20newsgroups(subset='train')\n","test_data = fetch_20newsgroups(subset='test')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DB8zuKnynvEs","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"4nWcJBtAMfcW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598189036819,"user_tz":-120,"elapsed":1648,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}}},"source":["def extract(d, keys):\n","    return dict((k, d[k]) for k in keys if k in d)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XnJs2kaPn1Ix","colab_type":"text"},"source":["Naive bayes is computationally really complex, so in order to fast up the training (and test) time i reduced the dataset size"]},{"cell_type":"code","metadata":{"id":"hT389hGnBqFF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1598189036822,"user_tz":-120,"elapsed":1636,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}},"outputId":"9d8c7418-97c9-4fd3-d3f4-0511ea0ed254"},"source":["import pandas as pd\n","train_data=extract(train_data,[\"data\",\"target\"])\n","train_data=pd.DataFrame.from_dict(train_data)\n","train_data=train_data.sample(frac=1,random_state=69)\n","train_X=train_data['data'].tolist()\n","train_y=train_data['target'].tolist()\n","\n","test_data=extract(test_data,[\"data\",\"target\"])\n","test_data=pd.DataFrame.from_dict(test_data)\n","#test_data=test_data.sample(1000,random_state=69)\n","test_X=test_data['data'].tolist()\n","test_y=test_data['target'].tolist()\n","\n","print(\"Number of train examples: \", len(train_X))\n","print(\"Number of test exaples: \", len(test_X))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Number of train examples:  11314\n","Number of test exaples:  7532\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0xjj6ZcZoRRH","colab_type":"text"},"source":["Define a method useful for printing out all the metrics useful to determine the quality of the classification"]},{"cell_type":"code","metadata":{"id":"Jcns1YJ5DU9v","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598189036825,"user_tz":-120,"elapsed":1623,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}}},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","\n","def evaluate(y_test, y_pred):\n","  print(\"accuracy:\", accuracy_score(y_test, y_pred))\n","  print(\"precision:\", precision_score(y_test, y_pred, average='micro'))\n","  print(\"recall:\", recall_score(y_test, y_pred, average='micro'))\n","  print(\"\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDyLeaB7ork3","colab_type":"text"},"source":["Connection to drive, necessary to acces to a couple of files like the vocabulary and the list of stopwords, necessary for the naive bayes algorithm"]},{"cell_type":"code","metadata":{"id":"E_hrAEn1ICLG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598189036827,"user_tz":-120,"elapsed":1611,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}},"outputId":"03aad620-1351-4e80-bfa9-9fdaf51ce7e5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vg9Fo-HQpMzE","colab_type":"text"},"source":["The files mentioned before are downloaded and transformed to a vector of words"]},{"cell_type":"code","metadata":{"id":"64fEVvdSI74Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598189036828,"user_tz":-120,"elapsed":1599,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}}},"source":["vocabulary_path=\"/content/drive/My Drive/Colab Notebooks/NaiveBayes/google-10000-english.txt\"\n","#vocabulary_path=\"/content/drive/My Drive/Colab Notebooks/NaiveBayes/english-3000.txt\"\n","vocabulary_file=open(vocabulary_path,\"r\")\n","vocabulary_text = vocabulary_file.read()\n","vocabulary=vocabulary_text.split()\n","vocabulary_file.close()\n","\n","stopwords_path=\"/content/drive/My Drive/Colab Notebooks/NaiveBayes/english-stopwords.txt\"\n","stopwords_file=open(stopwords_path,\"r\")\n","stopwords_text = stopwords_file.read()\n","stopwords=stopwords_text.split()\n","stopwords_file.close()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uxap6vDKpbRp","colab_type":"text"},"source":["# Naive Bayes algorithm implementation"]},{"cell_type":"code","metadata":{"id":"o91lBtipMpbo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598189036830,"user_tz":-120,"elapsed":1589,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}}},"source":["import numpy as np\n","import string\n","import sys\n","\n","class NaiveBayesTextClassifier:\n","  class_priors={}\n","  word_given_class={}\n","\n","  vocabulary=[]\n","\n","  def __init__(self,vocabulary, stopwords):\n","    self.vocabulary=list(set(vocabulary)-set(stopwords))\n","\n","  def train(self,X,y):\n","    document_by_class={}\n","    #Dividing documents by class  \n","    for i in range(0,len(y)):\n","      if y[i] not in document_by_class:\n","        document_by_class[y[i]]=[]\n","      document_by_class[y[i]].append(X[i])\n","\n","    #Esteeming the probabilities\n","    i=1\n","    for classification, documents in document_by_class.items():\n","      sys.stdout.write(\"\\r Calculating probability for classification number %i out of %i\" % (i ,len(document_by_class.keys())))\n","      i=i+1\n","      sys.stdout.flush()\n","      #Esteeming the priors of each value\n","      self.class_priors[classification]=len(documents)/len(y)\n","      #Esteeming the conditional probabilities of each document given the class\n","      words_count=dict((i,0) for i in vocabulary)\n","      self.word_given_class[classification]={}\n","      #Creating an unique document made by the conjunction of all documents\n","      unique_document=\" \".join(documents)\n","      #Preprocessing of the document\n","      unique_document=unique_document.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #removing all punctuation\n","      unique_document=unique_document.lower() #Converting to lowercase all the characters of the document\n","      words=unique_document.split()\n","      #Estimation of the conditional probabilties\n","      for word in words:\n","        if word in vocabulary:\n","          words_count[word]=words_count[word]+1\n","      for word, n_occurences in words_count.items():\n","        self.word_given_class[classification][word]=(n_occurences+1)/(len(words)+len(vocabulary))\n","\n","  def predict(self,document):\n","    best_class=None\n","    best_probability=None\n","    #Preprocessing of the document    \n","    temp_document=document.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) #removing all punctuation\n","    temp_document=temp_document.lower() #Converting to lowercase all the characters of the document\n","    words=temp_document.split()\n","    #Calculating the maximum likelihood for each classification\n","    for classification, word_probabilites in self.word_given_class.items():\n","      #Calculating such probabilities can bring really easilly to underflows due to the really small moltiplicated between each other\n","      #To avoid this problem instead of moltiplicating the probabilities we can sum the logarithms of each of them\n","      #The logarithm is in fact a monothone function so the results of the optimization problem doesn't change\n","      current_probability=np.log(self.class_priors[classification]) \n","      for word in words:\n","        if word in self.vocabulary:\n","          current_probability=current_probability+np.log(self.word_given_class[classification][word])\n","      if best_probability==None or current_probability>best_probability:\n","        best_class=classification\n","        best_probability=current_probability\n","    return best_class"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RbJ1HWJYpq9U","colab_type":"text"},"source":["# Algorithm Training"]},{"cell_type":"code","metadata":{"id":"rbddXls8FOAP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":313},"executionInfo":{"status":"error","timestamp":1598195957137,"user_tz":-120,"elapsed":16604,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}},"outputId":"a93ea91a-7669-4461-8dc3-4d695b4c673e"},"source":["classifier=NaiveBayesTextClassifier(vocabulary,stopwords)\n","classifier.train(train_X,train_y)"],"execution_count":10,"outputs":[{"output_type":"stream","text":[" Calculating probability for classification number 3 out of 20"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-dbbf9af2d1ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNaiveBayesTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-c2e6ef2e2073>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m#Estimation of the conditional probabilties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mwords_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_occurences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"PaeDnhOuqF0b","colab_type":"text"},"source":["# Algorithm Test"]},{"cell_type":"code","metadata":{"id":"GcC3bPCGXyLH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"status":"ok","timestamp":1598193482819,"user_tz":-120,"elapsed":4447550,"user":{"displayName":"tony visente","photoUrl":"","userId":"01943363031803034590"}},"outputId":"6e227646-8fa3-4bfe-df3a-44f8fd3aabd4"},"source":["predictions=[]\n","nTestExamples=len(test_X)\n","for i in range(0,nTestExamples):\n","  sys.stdout.write(\"\\r Calculating example %i out of %i\" % (i+1 ,nTestExamples))\n","  sys.stdout.flush()\n","  prediction=classifier.predict(test_X[i])\n","  predictions.append(prediction)\n","print()\n","predictions=[-1 if v is None else v for v in predictions]\n","evaluate(test_y[0:nTestExamples],predictions)"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" Calculating example 7532 out of 7532\n","accuracy: 0.7294211364843335\n","precision: 0.7294211364843335\n","recall: 0.7294211364843335\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1RB7EpGGqKK6","colab_type":"text"},"source":["# Conclusions"]},{"cell_type":"markdown","metadata":{"id":"DmIkd67EqQvv","colab_type":"text"},"source":["The results obtained are pretty decent, i think they could be improved even further by using a better vocabulary and a better list of stopwords"]}]}